import subprocess
import sys
import os
import time
import socket
from dotenv import load_dotenv

# ================= é…ç½®åŒºåŸŸ =================
FRONTEND_DIR = "src/frontend"  # å‰ç«¯ç›®å½•
MODEL_ID = "BAAI/bge-reranker-base" # ModelScope æ¨¡å‹ ID
MODELS_ROOT = "models" # æœ¬åœ°æ¨¡å‹å­˜æ”¾æ ¹ç›®å½•
# ===========================================

def wait_for_port(port, host='127.0.0.1', timeout=120, service_name="Service"):
    """
    æ£€æµ‹ç«¯å£æ˜¯å¦å¼€å¯ï¼ˆTCP Socket æ¢æµ‹ï¼‰
    """
    start_time = time.time()
    print(f"â³ ç­‰å¾… {service_name} (Port {port}) å°±ç»ª...", end="", flush=True)
    
    while True:
        try:
            with socket.create_connection((host, port), timeout=1):
                print(f" âœ… å°±ç»ªï¼")
                return True
        except (OSError, ConnectionRefusedError):
            if time.time() - start_time > timeout:
                print(f" âŒ è¶…æ—¶ï¼")
                return False
            time.sleep(0.5) # æ¯0.5ç§’æ£€æµ‹ä¸€æ¬¡
            print(".", end="", flush=True)

def check_and_download_model():
    """
    æ£€æŸ¥å¹¶ä½¿ç”¨ modelscope å‘½ä»¤è¡Œä¸‹è½½æ¨¡å‹
    """
    print(f"\nğŸ“¦ [Pre-check] æ£€æŸ¥æ¨¡å‹ç¯å¢ƒ...")
    
    # 1. æ£€æŸ¥è·¯å¾„ï¼šè®¡ç®—å®Œæ•´çš„æ¨¡å‹è·¯å¾„ç”¨äºæ£€æŸ¥ (models/BAAI/bge-reranker-base)
    full_model_path = os.path.join(MODELS_ROOT, *MODEL_ID.split("/"))

    # æ£€æŸ¥ config.json æ˜¯å¦å­˜åœ¨äºå®Œæ•´è·¯å¾„ä¸‹
    if os.path.exists(full_model_path) and os.path.exists(os.path.join(full_model_path, "config.json")):
        print(f"âœ… æ¨¡å‹å·²å­˜åœ¨äº: {full_model_path}")
        return

    # 2. å¦‚æœä¸å­˜åœ¨ï¼Œä¸‹è½½åˆ° models æ ¹ç›®å½•
    # æ³¨æ„ï¼šæ ¹æ® modelscope è¡Œä¸ºï¼ŒæŒ‡å®š --local_dir models ä¼šè‡ªåŠ¨åœ¨å…¶ä¸‹åˆ›å»º BAAI/bge-reranker-base
    print(f"â¬‡ï¸  æœªæ£€æµ‹åˆ°æ¨¡å‹ï¼Œæ­£åœ¨è°ƒç”¨å‘½ä»¤è¡Œä¸‹è½½: {MODEL_ID} ...")
    print(f"    ä¸‹è½½ç›®æ ‡æ ¹ç›®å½•: {MODELS_ROOT}")

    try:
        # ä½¿ç”¨ subprocess è°ƒç”¨å‘½ä»¤è¡Œ
        # check=True ä¼šåœ¨å‘½ä»¤è¿”å›éé›¶é€€å‡ºç æ—¶æŠ›å‡º CalledProcessError
        cmd = f"modelscope download --model {MODEL_ID} --local_dir {MODELS_ROOT}"
        subprocess.run(cmd, shell=True, check=True, env=os.environ)
        
        print("âœ… æ¨¡å‹ä¸‹è½½å‘½ä»¤æ‰§è¡Œå®Œæ¯•ï¼")
    except subprocess.CalledProcessError:
        print("âŒ é”™è¯¯: æ¨¡å‹ä¸‹è½½å¤±è´¥ã€‚")
        print("ğŸ‘‰ è¯·ç¡®ä¿å·²å®‰è£… modelscope å‘½ä»¤è¡Œå·¥å…·: pip install modelscope")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        sys.exit(1)

def check_and_install_frontend_deps():
    """
    æ£€æŸ¥å‰ç«¯ node_modules æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™æ‰§è¡Œ npm install
    """
    print(f"\nğŸ“¦ [Pre-check] æ£€æŸ¥å‰ç«¯ä¾èµ–...")
    
    # æ£€æŸ¥ node_modules æ˜¯å¦å­˜åœ¨
    node_modules_path = os.path.join(FRONTEND_DIR, "node_modules")
    
    if os.path.exists(node_modules_path) and os.path.isdir(node_modules_path):
        print(f"âœ… å‰ç«¯ä¾èµ– (node_modules) å·²å­˜åœ¨ï¼Œè·³è¿‡å®‰è£…ã€‚")
        return

    print(f"â¬‡ï¸  æœªæ£€æµ‹åˆ° node_modulesï¼Œæ­£åœ¨æ‰§è¡Œ npm install ...")
    print(f"    æ‰§è¡Œç›®å½•: {FRONTEND_DIR}")

    try:
        # åœ¨å‰ç«¯ç›®å½•ä¸‹æ‰§è¡Œ npm install
        # shell=True æ˜¯å¿…é¡»çš„ï¼Œä»¥ä¾¿åœ¨ Windows ä¸Šæ‰¾åˆ° npm å‘½ä»¤
        subprocess.run("npm install", cwd=FRONTEND_DIR, shell=True, check=True, env=os.environ)
        print("âœ… å‰ç«¯ä¾èµ–å®‰è£…å®Œæˆï¼")
    except subprocess.CalledProcessError:
        print("âŒ é”™è¯¯: npm install å¤±è´¥ã€‚")
        print("ğŸ‘‰ è¯·ç¡®ä¿å·²å®‰è£… Node.jsï¼Œæˆ–è€…å°è¯•æ‰‹åŠ¨è¿›å…¥ src/frontend æ‰§è¡Œ npm install")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        sys.exit(1)

def run_services():
    print("ğŸš€ [DeepResearch Agent] ä¸¥æ ¼é¡ºåºå¯åŠ¨è„šæœ¬")
    print("--------------------------------------------------")

    # 1. åŠ è½½ç¯å¢ƒå˜é‡
    print("ğŸ“‚ [Init] æ­£åœ¨åŠ è½½ .env ç¯å¢ƒå˜é‡...")
    load_dotenv(override=True)
    os.environ["PYTHONUTF8"] = "1"

    # ========================================================
    # é˜¶æ®µ 0: ç¯å¢ƒé¢„æ£€ (æ¨¡å‹ä¸‹è½½ & å‰ç«¯ä¾èµ–)
    # ========================================================
    check_and_download_model()
    check_and_install_frontend_deps()

    processes = []

    try:
        # ========================================================
        # é˜¶æ®µ 1: å¯åŠ¨ LiteLLM Proxy (Port 4000)
        # ========================================================
        print("\nğŸ¤– [1/3] æ­£åœ¨å¯åŠ¨ LiteLLM Proxy...")
        litellm_process = subprocess.Popen(
            ["litellm", "--config", "config.yaml"],
            shell=True,
            env=os.environ
        )
        processes.append(litellm_process)

        # â›”ï¸ é˜»å¡ç­‰å¾…ï¼šç›´åˆ° LiteLLM çš„ 4000 ç«¯å£é€šäº†ï¼Œæ‰ç»§ç»­
        if not wait_for_port(4000, service_name="LiteLLM"):
            raise RuntimeError("LiteLLM å¯åŠ¨å¤±è´¥ï¼Œç«¯å£æœªå“åº”ã€‚")

        # ========================================================
        # é˜¶æ®µ 2: å¯åŠ¨ FastAPI Backend (Port 8002)
        # ========================================================
        print("\nğŸ”Œ [2/3] æ­£åœ¨å¯åŠ¨ FastAPI (Backend)...")
        uvicorn_process = subprocess.Popen(
            [sys.executable, "-m", "uvicorn", "src.backend.api.server:app", "--port", "8002"],
            env=os.environ,
            shell=False
        )
        processes.append(uvicorn_process)

        # â›”ï¸ é˜»å¡ç­‰å¾…ï¼šç›´åˆ° Backend çš„ 8002 ç«¯å£é€šäº†ï¼Œæ‰ç»§ç»­
        if not wait_for_port(8002, service_name="FastAPI"):
            raise RuntimeError("FastAPI å¯åŠ¨å¤±è´¥ï¼Œç«¯å£æœªå“åº”ã€‚")

        # ========================================================
        # é˜¶æ®µ 3: å¯åŠ¨ Frontend (Port 5173 - é»˜è®¤ Vite ç«¯å£)
        # ========================================================
        print(f"\nğŸ’» [3/3] æ­£åœ¨å¯åŠ¨å‰ç«¯ (npm run dev)...")
        npm_cmd = "npm run dev -- --host 127.0.0.1 --port 5173"
        npm_process = subprocess.Popen(
            npm_cmd,
            cwd=FRONTEND_DIR,
            shell=True,
            env=os.environ
        )
        processes.append(npm_process)
        
        # å¯é€‰ï¼šä¹Ÿç­‰å¾…å‰ç«¯ç«¯å£å°±ç»ª
        if not wait_for_port(5173, service_name="Frontend"):
            raise RuntimeError("Frontend å¯åŠ¨å¤±è´¥ï¼Œç«¯å£æœªå“åº”ã€‚")

        print("\n--------------------------------------------------")
        print("âœ¨ å®Œç¾ï¼æ‰€æœ‰æœåŠ¡å·²æŒ‰é¡ºåºå¯åŠ¨å®Œæ¯•ã€‚")
        print("   1. LiteLLM Proxy: http://localhost:4000 (Ready)")
        print("   2. API Backend:   http://localhost:8002 (Ready)")
        print("   3. Frontend:      http://localhost:5173 (Running)")
        print("--------------------------------------------------")
        print("ğŸ‘‰ æŒ‰ Ctrl+C å¯ä»¥ä¸€æ¬¡æ€§åœæ­¢æ‰€æœ‰æœåŠ¡")

        # æŒ‚èµ·ä¸»è¿›ç¨‹
        uvicorn_process.wait()
        npm_process.wait()

    except KeyboardInterrupt:
        print("\n\nğŸ›‘ æ­£åœ¨åœæ­¢æ‰€æœ‰æœåŠ¡...")
    except RuntimeError as e:
        print(f"\nâŒ é”™è¯¯: {e}")
    finally:
        # æ¸…ç†é€»è¾‘
        for p in processes:
            try:
                p.terminate()
            except Exception:
                pass
            
            if p.poll() is None:
                subprocess.run(f"taskkill /F /T /PID {p.pid}", shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                
        print("âœ… æ‰€æœ‰æœåŠ¡å·²æ¸…ç†ã€‚")

if __name__ == "__main__":
    run_services()